{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning to count sequences of ones with RNNs\n",
    "\n",
    "Inspired from http://monik.in/a-noobs-guide-to-implementing-rnn-lstm-using-tensorflow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.12.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.  1.  0.  1.]\n",
      " [ 0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  1.]]\n",
      "[[ 4.]\n",
      " [ 1.]\n",
      " [ 2.]]\n"
     ]
    }
   ],
   "source": [
    "n_seqs = 100000\n",
    "seq_length = 5\n",
    "\n",
    "training_seqs = np.random.randint(2, size=(n_seqs, seq_length, 1)).astype(np.float64)\n",
    "targets = training_seqs.sum(axis=1, dtype=np.float64)\n",
    "\n",
    "print(training_seqs[:3, :, 0])\n",
    "\n",
    "print(targets[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = None\n",
    "\n",
    "sequence_input = tf.placeholder(np.float64, [BATCH_SIZE, seq_length, 1])\n",
    "target_input = tf.placeholder(np.float64, [BATCH_SIZE, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN model in TF\n",
    "As explained in http://monik.in/a-noobs-guide-to-implementing-rnn-lstm-using-tensorflow/:\n",
    "\n",
    " * We first define a basic [RNNCell](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/RNNCell) architecture with a given number of hiddent units\n",
    " * The function [dynamic_rnn](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn) then unrolls the recursion and return the hidden states $h_t$ for all time steps given an input sequence $x$.\n",
    " \n",
    "From http://colah.github.io/posts/2015-08-Understanding-LSTMs :\n",
    "\n",
    "![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.  1.  0.  1.]]\n",
      "[[[-0.12 -0.   -0.11 -0.08 -0.06 -0.04 -0.01 -0.01 -0.1  -0.  ]\n",
      "  [-0.21 -0.   -0.16 -0.13 -0.1  -0.09 -0.03  0.01 -0.15 -0.  ]\n",
      "  [-0.27 -0.01 -0.18 -0.17 -0.14 -0.12 -0.04  0.03 -0.19 -0.01]\n",
      "  [-0.21 -0.03 -0.09 -0.15 -0.12 -0.16 -0.04  0.1  -0.13 -0.01]\n",
      "  [-0.31 -0.04 -0.15 -0.14 -0.13 -0.12 -0.03  0.1  -0.16 -0.03]]]\n"
     ]
    }
   ],
   "source": [
    "num_hidden = 10\n",
    "# for 1.0 tf.contrib.rnn import LSTMCell\n",
    "cell = tf.nn.rnn_cell.LSTMCell(num_units=num_hidden)\n",
    "\n",
    "sequence_hidden_states, _ = tf.nn.dynamic_rnn(cell, inputs=sequence_input, dtype=np.float64, scope='unrolled_cells')\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "print(training_seqs[:1, :, 0])\n",
    "\n",
    "print(np.round(sequence_hidden_states.eval(feed_dict={sequence_input: training_seqs[:1]}), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the sum at the end of the sequence\n",
    "\n",
    "We are going to fit RNN cell inner weights (responsible to compute $h_t$) as well as the fully connected weights $W$ of size $H \\times 1$ \n",
    "$$ y = \\sum_t^T x_t \\approx h_T W$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-0.31, -0.04, -0.15, -0.14, -0.13, -0.12, -0.03, 0.1, -0.16, -0.03]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_state = tf.gather(tf.transpose(sequence_hidden_states, [1, 0, 2]), seq_length - 1)\n",
    "\n",
    "np.round(last_hidden_state.eval(feed_dict={sequence_input: training_seqs[:1]}) , 2).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.3097567]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = tf.Variable(tf.random_normal([num_hidden, 1], dtype=np.float64))\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "prediction = tf.matmul(last_hidden_state, W)\n",
    "\n",
    "prediction.eval(feed_dict={sequence_input: training_seqs[:1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Minimising the Mean Square error loss $ ||\\sum_t^T x_t - h_T W||_2$ on our training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7.9947950011152678"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_loss = tf.reduce_mean(tf.squared_difference(prediction, target_input))\n",
    "train_step = tf.train.AdamOptimizer().minimize(mse_loss)\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "mse_loss.eval(feed_dict={sequence_input: training_seqs[:2], target_input: targets[:2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 error 0.3\n",
      "Epoch  2 error 0.2\n",
      "Epoch  3 error 0.3\n",
      "Epoch  4 error 0.2\n",
      "Epoch  5 error 0.2\n",
      "Epoch  6 error 0.2\n",
      "Epoch  7 error 0.2\n",
      "Epoch  8 error 0.2\n",
      "Epoch  9 error 0.2\n",
      "Epoch 10 error 0.2\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 2048\n",
    "N_BATCHES = 10\n",
    "\n",
    "# may have to run several times\n",
    "for i in range(N_BATCHES):\n",
    "    batch_indexes = np.random.choice(n_seqs, BATCH_SIZE)\n",
    "    batch_loss, _ = sess.run(\n",
    "        [mse_loss, train_step],\n",
    "        {sequence_input: training_seqs[batch_indexes], target_input: targets[batch_indexes]})\n",
    "    \n",
    "    print('Epoch {:2d} error {:3.1f}'.format(i + 1, batch_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting predictions for any sequence\n",
    "\n",
    "It's neither very good or impressive! Possibly turning the prediction into a multi-class problem may help (as done in the initial blog post). \n",
    "\n",
    "The gist is that we are able to map a sequence of variable length into a fixed size representation that we can use as an input feature to a supervised task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.25796719]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.eval(feed_dict={sequence_input: np.array([1, 0, 0, 0, 1]).reshape((1, -1, 1))})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
