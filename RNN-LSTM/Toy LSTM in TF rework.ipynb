{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning to count sequences of ones with RNNs\n",
    "\n",
    "Inspired from http://monik.in/a-noobs-guide-to-implementing-rnn-lstm-using-tensorflow/\n",
    "\n",
    "Copy of https://github.com/pilipolio/python-ml-notebooks/blob/master/rnn/LSTM_toy.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.12.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  1.  0.]\n",
      " [ 1.  1.  1.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]]\n",
      "[[ 1.]\n",
      " [ 3.]\n",
      " [ 1.]]\n"
     ]
    }
   ],
   "source": [
    "n_seqs = 100000\n",
    "seq_length = 5\n",
    "\n",
    "training_seqs = np.random.randint(2, size=(n_seqs, seq_length, 1)).astype(np.float64)\n",
    "targets = training_seqs.sum(axis=1, dtype=np.float64)\n",
    "\n",
    "print(training_seqs[:3, :, 0])\n",
    "\n",
    "print(targets[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = None\n",
    "\n",
    "sequence_input = tf.placeholder(np.float64, [BATCH_SIZE, seq_length, 1])\n",
    "target_input = tf.placeholder(np.float64, [BATCH_SIZE, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN model in TF\n",
    "As explained in http://monik.in/a-noobs-guide-to-implementing-rnn-lstm-using-tensorflow/:\n",
    "\n",
    " * We first define a basic [RNNCell](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/RNNCell) architecture with a given number of hiddent units\n",
    " * The function [dynamic_rnn](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn) then unrolls the recursion and return the hidden states $h_t$ for all time steps given an input sequence $x$.\n",
    " \n",
    "From http://colah.github.io/posts/2015-08-Understanding-LSTMs :\n",
    "\n",
    "![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  1.  0.]]\n",
      "[[[ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      "  [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      "  [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      "  [-0.12 -0.08  0.04 -0.1  -0.07 -0.11  0.02  0.1  -0.07 -0.06]\n",
      "  [-0.11 -0.07  0.04 -0.07 -0.06 -0.07 -0.03  0.08 -0.05 -0.04]]]\n"
     ]
    }
   ],
   "source": [
    "num_hidden = 10\n",
    "# for 1.0 tf.contrib.rnn import LSTMCell\n",
    "cell = tf.nn.rnn_cell.LSTMCell(num_units=num_hidden)\n",
    "\n",
    "sequence_hidden_states, _ = tf.nn.dynamic_rnn(cell, inputs=sequence_input, dtype=np.float64, scope='unrolled_cells')\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "print(training_seqs[:1, :, 0])\n",
    "\n",
    "print(np.round(sequence_hidden_states.eval(feed_dict={sequence_input: training_seqs[:1]}), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the sum at the end of the sequence\n",
    "\n",
    "We are going to fit RNN cell inner weights (responsible to compute $h_t$) as well as the fully connected weights $W$ of size $H \\times 1$ \n",
    "$$ y = \\sum_t^T x_t \\approx h_T W$$\n",
    "\n",
    " * extract the last hidden state $h_T$ from `sequence_hidden_states` using `tf.gather` + `tf.transpose` (not obvious to be honest)\n",
    " * initialise a matrix of weights $W$ as a `tf.Variable`\n",
    " * compute the prediction tensor as the `tf.matmul` of $h_T$ and $W$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-0.11, -0.07, 0.04, -0.07, -0.06, -0.07, -0.03, 0.08, -0.05, -0.04]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_state = tf.gather(tf.transpose(sequence_hidden_states, [1, 0, 2]), seq_length - 1)\n",
    "\n",
    "np.round(last_hidden_state.eval(feed_dict={sequence_input: training_seqs[:1]}) , 2).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07879528]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = tf.Variable(tf.random_normal([num_hidden, 1], dtype=np.float64))\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "prediction = tf.matmul(last_hidden_state, W)\n",
    "\n",
    "prediction.eval(feed_dict={sequence_input: training_seqs[:1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Minimising the Mean Square error loss $ ||\\sum_t^T x_t - h_T W||_2$ on our training data set\n",
    " \n",
    " * define the loss between `prediction` tensor and `target_input` placeholder using `tf.reduce_mean` and `tf.squared_difference`\n",
    " * define a `training_step` as `optimizer.minimize(...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olegshevelev/Soft/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.9494932067422415"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_loss = tf.reduce_mean(tf.squared_difference(prediction, target_input))\n",
    "train_step = tf.train.AdamOptimizer().minimize(mse_loss)\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "mse_loss.eval(feed_dict={sequence_input: training_seqs[:2], target_input: targets[:2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 error 8.3\n",
      "Epoch  2 error 8.2\n",
      "Epoch  3 error 7.9\n",
      "Epoch  4 error 7.8\n",
      "Epoch  5 error 8.0\n",
      "Epoch  6 error 7.8\n",
      "Epoch  7 error 7.8\n",
      "Epoch  8 error 7.6\n",
      "Epoch  9 error 7.5\n",
      "Epoch 10 error 7.5\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 2048\n",
    "N_BATCHES = 10\n",
    "\n",
    "# may have to run several times\n",
    "for i in range(N_BATCHES):\n",
    "    batch_indexes = np.random.choice(n_seqs, BATCH_SIZE)\n",
    "    batch_loss, _ = sess.run(\n",
    "        [mse_loss, train_step],\n",
    "        {sequence_input: training_seqs[batch_indexes], target_input: targets[batch_indexes]})\n",
    "    \n",
    "    print('Epoch {:2d} error {:3.1f}'.format(i + 1, batch_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting predictions for any sequence\n",
    "\n",
    "It's neither very good or impressive! Possibly turning the prediction into a multi-class problem may help (as done in the initial blog post). \n",
    "\n",
    "The gist is that we are able to map a sequence of variable length into a fixed size representation that we can use as an input feature to a supervised task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01487602]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.eval(feed_dict={sequence_input: np.array([1, 0, 0, 0, 1]).reshape((1, -1, 1))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
