{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorize nationality of last names by RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-03-24 14:41:06--  https://download.pytorch.org/tutorial/data.zip\n",
      "Resolving download.pytorch.org... 52.222.236.22, 52.222.236.252, 52.222.236.154, ...\n",
      "Connecting to download.pytorch.org|52.222.236.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2882130 (2.7M) [application/zip]\n",
      "Saving to: 'data.zip'\n",
      "\n",
      "data.zip            100%[===================>]   2.75M  2.06MB/s    in 1.3s    \n",
      "\n",
      "2017-03-24 14:41:07 (2.06 MB/s) - 'data.zip' saved [2882130/2882130]\n",
      "\n",
      "Archive:  data.zip\n",
      "  inflating: data/eng-fra.txt        \n",
      "   creating: data/names/\n",
      "  inflating: data/names/Arabic.txt   \n",
      "  inflating: data/names/Chinese.txt  \n",
      "  inflating: data/names/Czech.txt    \n",
      "  inflating: data/names/Dutch.txt    \n",
      "  inflating: data/names/English.txt  \n",
      "  inflating: data/names/French.txt   \n",
      "  inflating: data/names/German.txt   \n",
      "  inflating: data/names/Greek.txt    \n",
      "  inflating: data/names/Irish.txt    \n",
      "  inflating: data/names/Italian.txt  \n",
      "  inflating: data/names/Japanese.txt  \n",
      "  inflating: data/names/Korean.txt   \n",
      "  inflating: data/names/Polish.txt   \n",
      "  inflating: data/names/Portuguese.txt  \n",
      "  inflating: data/names/Russian.txt  \n",
      "  inflating: data/names/Scottish.txt  \n",
      "  inflating: data/names/Spanish.txt  \n",
      "  inflating: data/names/Vietnamese.txt  \n",
      "eng-fra.txt   \u001b[34mnames\u001b[m\u001b[m         \u001b[34mopensubtitles\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!wget https://download.pytorch.org/tutorial/data.zip\n",
    "! unzip data.zip\n",
    "! ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/names/Arabic.txt', 'data/names/Chinese.txt', 'data/names/Czech.txt', 'data/names/Dutch.txt', 'data/names/English.txt', 'data/names/French.txt', 'data/names/German.txt', 'data/names/Greek.txt', 'data/names/Irish.txt', 'data/names/Italian.txt', 'data/names/Japanese.txt', 'data/names/Korean.txt', 'data/names/Polish.txt', 'data/names/Portuguese.txt', 'data/names/Russian.txt', 'data/names/Scottish.txt', 'data/names/Spanish.txt', 'data/names/Vietnamese.txt']\n",
      "Slusarski\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "print(findFiles('data/names/*.txt'))\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "print(unicodeToAscii('Ślusàrski'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename).read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category = filename.split('/')[-1].split('.')[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)\n",
    "n_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Arabic', 'Chinese', 'Czech', 'Dutch', 'English', 'French', 'German',\n",
       "       'Greek', 'Irish', 'Italian', 'Japanese', 'Korean', 'Polish',\n",
       "       'Portuguese', 'Russian', 'Scottish', 'Spanish', 'Vietnamese'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "name_nations = pd.DataFrame([\n",
    "    (nation,name)\n",
    "    for nation, list_of_names in category_lines.items()\n",
    "    for name in list_of_names], columns=['nation', 'name']\n",
    ")\n",
    "\n",
    "name_nations = name_nations\\\n",
    "    .assign(nation=lambda df: df.nation.astype('category'))\\\n",
    "    .assign(y=lambda df: df.nation.cat.codes\n",
    ")\n",
    "name_nations.nation.cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_name_length = name_nations.name.apply(len).max()\n",
    "max_name_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.]]\n",
      "(19, 58)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Find letter index from all_letters, e.g. \"a\" = 1\n",
    "# we'll reserve 0 for padding symbol\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter) + 1\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = np.zeros(shape=(1, n_letters + 1))\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line, max_length=max_name_length):\n",
    "    tensor = np.zeros(shape=(max_length, n_letters + 1))\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "print(letterToTensor('J'))\n",
    "\n",
    "print(lineToTensor('Jones').shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20074, 19, 58)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_seqs = np.stack([lineToTensor(name) for name in name_nations.name])\n",
    "training_seqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.]]\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "tf.__version__\n",
    "\n",
    "seq_length = max_name_length\n",
    "targets = pd.get_dummies(name_nations.y).values\n",
    "\n",
    "print(training_seqs[:3, :, 0])\n",
    "print(targets[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "num_hidden = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(num_hidden, input_shape=(max_name_length, n_letters + 1), activation='sigmoid', inner_activation='hard_sigmoid'))\n",
    "model.add(Dense(n_categories))\n",
    "model.add(Activation('softmax'))\n",
    "optimizer = Adam(lr=0.005)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10s - loss: 1.2833 - acc: 0.6331\n",
      "Epoch 2/10\n",
      "11s - loss: 1.2393 - acc: 0.6496\n",
      "Epoch 3/10\n",
      "11s - loss: 1.2187 - acc: 0.6540\n",
      "Epoch 4/10\n",
      "11s - loss: 1.1846 - acc: 0.6652\n",
      "Epoch 5/10\n",
      "10s - loss: 1.1588 - acc: 0.6669\n",
      "Epoch 6/10\n",
      "11s - loss: 1.1278 - acc: 0.6765\n",
      "Epoch 7/10\n",
      "10s - loss: 1.0729 - acc: 0.6946\n",
      "Epoch 8/10\n",
      "10s - loss: 1.0492 - acc: 0.6995\n",
      "Epoch 9/10\n",
      "10s - loss: 1.0077 - acc: 0.7099\n",
      "Epoch 10/10\n",
      "10s - loss: 0.9767 - acc: 0.7170\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x138ac5908>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(training_seqs, targets, batch_size=500, nb_epoch=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_nationality(name):\n",
    "    result = model.predict(np.array([lineToTensor(name)]), verbose=0)\n",
    "    return name_nations.nation.cat.categories[np.argmax(result)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'English'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_nationality(\"Rochefort\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'English'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_nationality(\"Kon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'English'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_nationality(\"Allain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Russian'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_nationality(\"Soldatov\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'English'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_nationality(\"Lu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
